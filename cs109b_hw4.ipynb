{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hhuangt/Intro-to-Data-Scientist-in-Python/blob/master/cs109b_hw4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4HAbin1bO5iS"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook(\"cs109b_hw4.ipynb\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmO29d4ux0Lq"
      },
      "source": [
        "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science\n",
        "## Homework 4: Convolutional Neural Networks\n",
        "\n",
        "\n",
        "**Harvard University**<br/>\n",
        "**Spring 2024**<br/>\n",
        "**Instructors**: Pavlos Protopapas & Alex Young\n",
        "\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "TPTLfywkx0Lw",
        "outputId": "7298e79c-f677-4cdb-a02d-e99e4ea1b8f6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "blockquote { background: #AEDE94; }\n",
              "h1 { \n",
              "    padding-top: 25px;\n",
              "    padding-bottom: 25px;\n",
              "    text-align: left; \n",
              "    padding-left: 10px;\n",
              "    background-color: #DDDDDD; \n",
              "    color: black;\n",
              "}\n",
              "h2 { \n",
              "    padding-top: 10px;\n",
              "    padding-bottom: 10px;\n",
              "    text-align: left; \n",
              "    padding-left: 5px;\n",
              "    background-color: #EEEEEE; \n",
              "    color: black;\n",
              "}\n",
              "\n",
              "div.exercise {\n",
              "\tbackground-color: #ffcccc;\n",
              "\tborder-color: #E9967A; \t\n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "}\n",
              "\n",
              "div.exercise-r {\n",
              "\tbackground-color: #fce8e8;\n",
              "\tborder-color: #E9967A; \t\n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "}\n",
              "\n",
              "\n",
              "span.sub-q {\n",
              "\tfont-weight: bold;\n",
              "}\n",
              "div.theme {\n",
              "\tbackground-color: #DDDDDD;\n",
              "\tborder-color: #E9967A; \t\n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "\tfont-size: 18pt;\n",
              "}\n",
              "div.gc { \n",
              "\tbackground-color: #AEDE94;\n",
              "\tborder-color: #E9967A; \t \n",
              "\tborder-left: 5px solid #800080; \n",
              "\tpadding: 0.5em;\n",
              "\tfont-size: 12pt;\n",
              "}\n",
              "p.q1 { \n",
              "    padding-top: 5px;\n",
              "    padding-bottom: 5px;\n",
              "    text-align: left; \n",
              "    padding-left: 5px;\n",
              "    background-color: #EEEEEE; \n",
              "    color: black;\n",
              "}\n",
              "header {\n",
              "   padding-top: 35px;\n",
              "    padding-bottom: 35px;\n",
              "    text-align: left; \n",
              "    padding-left: 10px;\n",
              "    background-color: #DDDDDD; \n",
              "    color: black;\n",
              "}\n",
              "</style>\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# RUN THIS CELL\n",
        "import requests\n",
        "from IPython.core.display import HTML\n",
        "styles = requests.get(\n",
        "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
        "    \"content/styles/cs109.css\"\n",
        ").text\n",
        "HTML(styles)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_P-gBWJQD6X",
        "outputId": "e87477b7-349b-429c-a80f-656407e327a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.0)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow-addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf-keras-vis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ml8v5g6oQSEa",
        "outputId": "f1515da4-edcd-4f3b-c985-18b5b8e99421"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-keras-vis\n",
            "  Downloading tf_keras_vis-0.8.6-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m607.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from tf-keras-vis) (1.11.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-keras-vis) (9.4.0)\n",
            "Collecting deprecated (from tf-keras-vis)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.10/dist-packages (from tf-keras-vis) (2.31.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-keras-vis) (24.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->tf-keras-vis) (1.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from imageio->tf-keras-vis) (1.25.2)\n",
            "Installing collected packages: deprecated, tf-keras-vis\n",
            "Successfully installed deprecated-1.2.14 tf-keras-vis-0.8.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HaPrtFjfx0L1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import time\n",
        "import zipfile\n",
        "import tarfile\n",
        "\n",
        "import imageio\n",
        "from matplotlib import cm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "from PIL import Image\n",
        "import requests\n",
        "import scipy.ndimage as ndimage\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Activation, BatchNormalization\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Dropout, GaussianNoise\n",
        "from tensorflow.keras.layers import Flatten, Input, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.regularizers import L1, L2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback, ModelCheckpoint\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tf_keras_vis.gradcam import Gradcam\n",
        "from tf_keras_vis.saliency import Saliency\n",
        "from tf_keras_vis.utils import normalize\n",
        "import tqdm\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfmuUJcLO5iU"
      },
      "outputs": [],
      "source": [
        "# measure notebook runtime\n",
        "time_start = time.time()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVkno877O5iU"
      },
      "source": [
        "<div style = \"background: lightsalmon; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
        "\n",
        "### Instructions\n",
        "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
        "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should includelabels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
        "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
        "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
        "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
        "- Enable scrolling output on cells with very long output.\n",
        "- Feel free to add additional code or markdown cells as needed.\n",
        "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes).\n",
        "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soOeL1Tpx0L4"
      },
      "source": [
        "**Please run these cells below.** First we set our local working directory to ensure our provided code in PART 2 works correctly, and then we confirm [TensorFlow eager execution](https://www.tensorflow.org/guide/eager) and print a summary of whether there are local GPUs available for training your models. Running this HW on JupyterHub is recommended. But as long as you are using the provided `cs109b.yml` conda environment, you should expect to see a TensorFlow version >=2.3.0, which should allow this notebook to run without error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-UPOjnXx0L6"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL\n",
        "\n",
        "working_dir = pathlib.Path().absolute()\n",
        "# Uncomment line below to debug if images don't show\n",
        "#print(working_dir)\n",
        "os.chdir(working_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M08r1qFTx0L8"
      },
      "outputs": [],
      "source": [
        "# RUN THIS CELL\n",
        "\n",
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment\n",
        "# that evaluates operations immediately, without building graphs\n",
        "\n",
        "print(f\"tensorflow version {tf.__version__}\")\n",
        "print(f\"Eager Execution Enabled: {tf.executing_eagerly()}\\n\")\n",
        "\n",
        "devices = tf.config.get_visible_devices()\n",
        "print(f\"All Devices: \\n{devices}\\n\")\n",
        "print(f\"Available GPUs: \\n{tf.config.list_logical_devices('GPU')}\\n\")\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/data_performance\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "tf.random.set_seed(2266)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7AgX8bWx0L-"
      },
      "source": [
        "\n",
        "<a id=\"contents\"></a>\n",
        "\n",
        "## Notebook Contents\n",
        "\n",
        "- [**PART 1: Building a Basic CNN Model**](#part1)\n",
        "\n",
        "- [**PART 2: Regression with CNN**](#part2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzGonykqx0L_"
      },
      "source": [
        "## About this Homework"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L555gy_1x0L_"
      },
      "source": [
        "In this homework, we will explore Convolutional Neural Networks (CNNs).\n",
        "\n",
        "- In [PART 1](#part1), we will begin by building a CNN to classify CIFAR-10 images, a standard pedagogical problem.\n",
        "\n",
        "\n",
        "- Then, in [PART 2](#part2), we will then see that CNNs are great for more than just classifying our images! They can serve as image input processing for a variety of tasks, as we will show by training a network on the CelebA dataset to rotate images of faces upright.\n",
        "\n",
        "**IMPORTANT NOTES:**\n",
        "\n",
        "- Convolutional neural networks are computationally intensive.\n",
        "- **We highly recommend that you train your model on a system using GPUs. For this, we recommend using the GPU-enabled JupyterHub environment provided to you as part of this course** (or you could also take a look at Google Colab's runtime settings for accessing a GPU-enabled environment free of cost).\n",
        "- Models that take hours to train on CPUs can be trained in just minutes when using GPUs.\n",
        "- Additionally, **if you become frustrated having to rerun your model every time you open your notebook, take a look at how to save your trained model weights for later use** (as is required in [PART 2, question 2.2.3](#q223)).\n",
        "\n",
        "**KERNEL CRASHES:**\n",
        "\n",
        "If your kernel crashes as you attempt to train your model, please check the following items:\n",
        "- Models with too many parameters might not fit in GPU memory. Try reducing the size of your model.\n",
        "- A large `batch_size` will attempt to load too many images in GPU memory. Avoid using a very large batch size.\n",
        "- Avoid creating multiple copies of the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JOHqvz5Ux0ME",
        "tags": []
      },
      "source": [
        "<a id=\"part1\"></a>\n",
        "    \n",
        "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
        "\n",
        "# PART 1: Building a Basic CNN Model\n",
        "\n",
        "\n",
        "<a id=\"part1intro\"></a>\n",
        "\n",
        "## Overview\n",
        "\n",
        "[Return to contents](#contents)\n",
        "\n",
        "In this question, you will use Keras to create a convolutional neural network for predicting the \"type of object\" shown in each image from the [CIFAR-10](https://keras.io/datasets/#cifar10-small-image-classification) dataset. This dataset contains 50,000 32x32 colored training images and 10,000 test images of the same size, with a total of 10 classes, representing the \"type of object\" shown in each image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ozy2F1peO5iV"
      },
      "source": [
        "<a id=\"q11\"></a>\n",
        "\n",
        "\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'>\n",
        "    \n",
        "<b>1.1 Loading CIFAR-10 and Constructing the Model</b>\n",
        "<hr>\n",
        "<b>Q1.1.1 - Preprocessing</b>\n",
        "\n",
        "<a id=\"q111\"></a>\n",
        "\n",
        "Load the CIFAR-10 dataset from the `tensorflow.keras.datasets.cifar10` import shown at the top of this notebook. Perform any preprocessing of the data that might be required for this dataset.\n",
        "    \n",
        "You may choose to load cifar10 as either a numpy array or as a Tensorflow Dataset.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21P6WuWVx0MJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dX0OlDNcx0MF",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q1.1.2</b>\n",
        "\n",
        "<a id=\"q112\"></a>\n",
        "\n",
        "Construct a classification model architecture using a combination of the following layers: Conv2D, MaxPooling2D, Dense, Dropout and Flatten. The layers don’t necessarily need to be in this order, and you can use as many of these types of layers as you’d like.\n",
        "\n",
        "  - You may choose to construct your own implementation of a well-known architecture like AlexNet or VGG16, or you can create an architecture of your own devising.\n",
        "\n",
        "  - However, you MUST code the network yourself and not use a pre-written implementation.\n",
        "\n",
        "  - You must have at least 2 Conv2D layers, and at least one of your Conv2D layers should have 9 or more filters in order to complete question 1.4.1.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0CvnFEcux0MK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "HyIGbNIMO5iV"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q1.2 Model parameter growth</b>\n",
        "\n",
        "<a id=\"q12\"></a>\n",
        "\n",
        "How does the number of total parameters change (e.g. linearly, exponentially, etc.) as the number of filters per layer increases (your model should have at least 2 Conv layers)? You can find this empirically by constructing multiple models with the same type of architecture and increasing the number of filters. Generate a plot showing the relationship and explain why it has this relationship.\n",
        "\n",
        "**HINT:** Completing this question is far easier if you write a function that generates your desired architecture in 1.1.2, with arguments that allow you to easily rebuild the architecture with varying numbers of filters per layer.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wnS8zEOmx0MM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kyquiKbx0MM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "fYRNC058O5iV"
      },
      "source": [
        "<a id=\"q13\"></a>\n",
        "\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'>\n",
        "\n",
        "<b>1.3 Choose a model, train and evaluate it</b>\n",
        "<hr>    \n",
        "<b>Q1.3.1</b>\n",
        "\n",
        "<a id=\"q131\"></a>\n",
        "\n",
        "Print the model summary for your chosen architecture, and report the total number of parameters. Then train your model using the CIFAR-10 dataset, and `validation_split=0.2`. You can choose to train your model for as long as you'd like, but you should aim for at least 10 epochs. Report your validation and test accuracies. They should both exceed 70%.\n",
        "\n",
        "**Hint:** It would be helpful to add code which either saves your model to a local directory if it is the first time you're training it or loads your model if a saved file version currently exists in that directory. This will not only help save time when you rerun your notebook, but it will also ensure reproducible results in the rest of Part 1.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZHkmUJgix0MP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JCEMhBKGx0MQ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW_PvxfFx0MR",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EabVyIxhx0MS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qoyIWiVjO5iV"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q1.3.2</b>\n",
        "\n",
        "<a id=\"q132\"></a>\n",
        "\n",
        "Plot the training loss and accuracy per epoch (both train and validation) for your chosen architecture.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwXHX6i8x0MT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCho2YBjx0MT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic1uIduQx0Me"
      },
      "source": [
        "<a id=\"part2\"></a>\n",
        "    \n",
        "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
        "\n",
        "# PART 2: Regression with CNN\n",
        "<a id=\"part2intro\"></a>\n",
        "## Overview\n",
        "\n",
        "[Return to contents](#contents)\n",
        "\n",
        "**In this problem we will construct a neural network to predict how far a face is from being \"upright\"**.\n",
        "\n",
        "**Image orientation estimation**\n",
        "\n",
        "Image orientation estimation with convolutional networks was first implemented in 2015 by Fischer, Dosovitskiy, and Brox in a paper titled [\"Image Orientation Estimation with Convolutional Networks\"](https://lmb.informatik.uni-freiburg.de/Publications/2015/FDB15/image_orientation.pdf). In that paper, the authors trained a network to straighten a wide variety of images using the [Microsoft COCO dataset](https://cocodataset.org/#home).\n",
        "\n",
        "**The modified CelebA dataset**\n",
        "\n",
        "In order to have a reasonable training time for a homework, we will be working on a subset of the problem where we just straighten images of faces. To do this:\n",
        "\n",
        "- We will be using the [CelebA](http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) dataset of celebrity faces, where we assume that professional photographers have taken level pictures;\n",
        "\n",
        "\n",
        "- The training will be supervised, with a rotated image (up to $\\pm 60^\\circ$) as an input, and the amount (in degrees) that the image has been rotated as a target."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "y18d8i2OO5iW"
      },
      "source": [
        "<a id=\"q21\"></a>\n",
        "\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'>\n",
        "<b>2.1 Data preparation</b>\n",
        "<hr>\n",
        "<b>Q2.1.1</b>\n",
        "\n",
        "<a id=\"q211\"></a>\n",
        "\n",
        "Loading CelebA and Thinking about Datasets.** Run the cells provided to automatically download the CelebA dataset. It is about 1.3GB, which can take 10-15 minutes to download. This happens only once. In the future, when you rerun the cell, it will use the dataset already stored on your machine. Once downloaded, we load the CelebA image data as a TensorFlow Dataset object. [TensorFlow Datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) are objects that behave like Python generators, allowing you to take elements (either input/target tuples or feature dictionaries) until you have gone through the entire dataset. Note how this is different from PART 1 where the entire dataset was loaded in as an array. Datasets also allow you to pipeline transformations to be applied to the elements, resulting in a new transformed Dataset (like `train_rot_ds`).\n",
        "\n",
        "  - **Run the provided code**:\n",
        "\n",
        "    - The creation of the normalization/rotation/resize pipeline has been done for you, resulting in train dataset `train_rot_ds` and validation dataset `test_rot_ds`.\n",
        "  \n",
        "  - **Answer this question:**\n",
        "  \n",
        "    - Aside from pipelining, what is an important practical reason to use TensorFlow Dataset objects over simply loading all the data into $X$ and $Y$ `numpy` arrays?\n",
        "    \n",
        "**Note:** You do not need to create a separate validation anywhere in Part 2. We are just using train and test with no validation for simplicity.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-zzEjAxXO5iW"
      },
      "outputs": [],
      "source": [
        "# Run this cell to define our download_celeb function\n",
        "def download_celeb(\n",
        "    url,\n",
        "    filename,\n",
        "    filepath,\n",
        "    dirname,\n",
        "    dirpath,\n",
        "    chunk_size=1204,\n",
        "    overwrite=False,\n",
        "):\n",
        "    \"\"\"Downloads and extracts CelebA dataset from CS109B S3 bucket\"\"\"\n",
        "\n",
        "    # Do not download if data already exists and overwrite==False\n",
        "    if not overwrite and os.path.isdir(os.path.join(dirpath, \"2.1.0\")):\n",
        "        print(\n",
        "            \"Congratulations...the CelebA dataset already exists \"\n",
        "            \"locally!\\nNo new downloads are required :o)\\n\"\n",
        "        )\n",
        "    # Download and extract CelebA if it doesn't already exist\n",
        "    else:\n",
        "        print(\"Downloading CelebA dataset to {}\\n\".format(filepath))\n",
        "\n",
        "        with requests.get(url, stream=True) as r:\n",
        "            chunk_size = 1024\n",
        "            print(\n",
        "                \"...downloading\"\n",
        "            )\n",
        "            time.sleep(0.5)\n",
        "            with open(filepath, 'wb') as f:\n",
        "                for chunk in tqdm.tqdm(\n",
        "                    r.iter_content(chunk_size=chunk_size),\n",
        "                    unit=\"KB\"\n",
        "                ):\n",
        "                    f.write(chunk)\n",
        "\n",
        "        print(\"...{} download complete :o\".format(filename))\n",
        "\n",
        "        if not os.path.isdir(dirpath):\n",
        "            os.makedirs(dirpath)\n",
        "\n",
        "        print(\n",
        "            \"...extracting {}. This will take a while too :o\\n\"\n",
        "            \"\".format(filename)\n",
        "        )\n",
        "\n",
        "        import tarfile\n",
        "        with tarfile.open(filepath, \"r:gz\") as tar:\n",
        "            tar.extractall(path=dirpath)\n",
        "\n",
        "        print(\n",
        "            \"The CelebA dataset has been extracted to:\"\n",
        "            \"\\n\\n\\t{}\\n\".format(dirpath)\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "LgXME64kO5iW"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Download the CelebA dataset\n",
        "url = \"https://bit.ly/4a79WzJ\"\n",
        "filename = \"2.1.0.tar.gz\"\n",
        "dirname = \"data\"\n",
        "dirpath = os.path.join(working_dir, dirname)\n",
        "filepath = os.path.join(working_dir, filename)\n",
        "\n",
        "# Running on JupyterHub with data\n",
        "if os.path.isdir('/home/course_data/celeb_a/2.1.0/'):\n",
        "    data_dir = '/home/course_data'\n",
        "# Running anywhere else\n",
        "else:\n",
        "    data_dir = os.path.join(working_dir, \"data\")\n",
        "    download_celeb(url, filename, filepath, dirname, dirpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Q2VdO2qxO5iW"
      },
      "outputs": [],
      "source": [
        "# This will load the dataset\n",
        "# Either the one hosted on Jupyter Hub or the copy you downloaded above\n",
        "train_celeb, test_celeb = tfds.load(\n",
        "    \"celeb_a\",\n",
        "    split=[\"train\", \"test\"],\n",
        "    shuffle_files=False,\n",
        "    data_dir = data_dir,\n",
        "    download=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6vrAkTQSx0Ml"
      },
      "outputs": [],
      "source": [
        "# You may use the following two functions\n",
        "def normalize_image(img):\n",
        "    return tf.cast(img, tf.float32)/255.\n",
        "\n",
        "def rot_resize(img, deg):\n",
        "    rotimg = ndimage.rotate(img, deg, reshape=False, order=3)\n",
        "    rotimg = np.clip(rotimg, 0., 1.)\n",
        "    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n",
        "    return rotimg\n",
        "\n",
        "################################################################\n",
        "# Don't manually invoke these functions; they are for Dataset\n",
        "# pipelining that is already done for you.\n",
        "################################################################\n",
        "def tf_rot_resize(img, deg):\n",
        "    \"\"\"Dataset pipe that rotates an image and resizes it to 140x120\"\"\"\n",
        "    rotimg = tfa.image.rotate(img, deg/180.*np.pi, interpolation=\"BILINEAR\")\n",
        "    rotimg = tf.image.resize_with_crop_or_pad(rotimg,140,120)\n",
        "    return rotimg\n",
        "\n",
        "def tf_random_rotate_helper(image):\n",
        "    \"\"\"Dataset pipe that normalizes image to [0.,1.] and rotates by a random\n",
        "    amount of degrees in [-60.,60.], returning an (input,target) pair consisting\n",
        "    of the rotated and resized image and the degrees it has been rotated by.\"\"\"\n",
        "    image = normalize_image(image)\n",
        "    deg = tf.random.uniform([],-60.,60.)\n",
        "    return (tf_rot_resize(image,deg), deg)  # (data, label)\n",
        "\n",
        "def tf_random_rotate_image(element):\n",
        "    \"\"\"Given an element drawn from the CelebA dataset, this returns a rotated\n",
        "    image and the amount it has been rotated by, in degrees.\"\"\"\n",
        "    image = element['image']\n",
        "    image, label = tf_random_rotate_helper(image)\n",
        "    image.set_shape((140,120,3))\n",
        "    return image, label\n",
        "################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "VYXdRVY8x0Mm"
      },
      "outputs": [],
      "source": [
        "# Pipeline for creating randomly rotated images with their target labels being\n",
        "# the amount they were rotated, in degrees.\n",
        "train_rot_ds = train_celeb.map(tf_random_rotate_image)\n",
        "test_rot_ds = test_celeb.map(tf_random_rotate_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5LoRtvfjO5iW"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q2.1.2</b>\n",
        "\n",
        "<a id=\"q212\"></a>\n",
        "\n",
        "**Taking a look.** In a grid of subplots, plot at least 4 rotated images from `train_rot_ds` with the titles being the amount the images have been rotated. The floating point numbers in the titles should have a reasonable number of digits. **HINT:** one way to get a few image+label tuples from the Dataset is with `train_rot_ds.take(4)`. Check the [TensorFlow Datasets documentation](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) for more information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHy76uiox0Mo",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "-V3cWPARx0Mf"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q2.1.3</b>\n",
        "\n",
        "<a id=\"q213\"></a>\n",
        "\n",
        "**2.1.3** **Conceptual Question.** Dropout layers have been shown to work well for regularizing deep neural networks, and can be used with very little computational cost. For our network, is it a good idea to use dropout layers? Explain, in **3-5 sentences**, being sure to explicitly discuss how a dropout layer works, and what that would mean for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mnvKlsejO5iW"
      },
      "source": [
        "<a id=\"q22\"></a>\n",
        "\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'>\n",
        "<b>2.2 Building and training your CNN</b>\n",
        "<hr>\n",
        "<a id=\"q221\"></a>\n",
        "<b>Q2.2.1 Compiling your model.</b>\n",
        "    \n",
        "Construct a model with multiple Conv layers and any other layers you think would help. Be certain to print your model summary as always. Feel free to experiment with architectures and number of parameters if you wish to get better performance or better training speed. You certainly don't need more than a few million parameters; we have been able to do it with substantially fewer.\n",
        "    \n",
        "**Note:** Again, it is fine to attempt your own implementation of a well-known architecture, but you may not load any pre-constructed models. The network must be built layer-by-layer from your own code.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaD3HlKNx0Mr",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPGCc4JJx0Ms",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "rB8uZ4QIO5iX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q2.2.2 Training your model</b>\n",
        "\n",
        "<a id=\"q222\"></a>\n",
        "\n",
        "Train your model. Please note that the `model.fit()` argument syntax is a little different when working with Datasets instead of numpy arrays; take a look at the [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) documentation. Be sure to also pass the test data as validation data. When passing `train_rot_ds` to `fit()`, you will find it useful to use pipelines to [batch](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch) the data. You can also experiment with [prefetching](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) batches/elements from the dataset, which may allow you to speed up iterations by a few percent. Finally, while dry-running and prototyping your model, you may find it useful to [take](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#take) a subset of the data to speed up experimentation. However, your final model MUST be trained on all the available training data! You should achieve test MSEs of less than 9, corresponding roughly to $\\pm 3^\\circ$ accuracy in predicting the rotations on the test set. This can be achieved in just 2-3 epochs, though you are free to train as long as you want.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_shHXiHx0Mt",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMdu9sKKx0Mt",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP48d4kox0Mu",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "GFDrr4BCO5iX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q2.2.3 Saving and loading your weights</b>\n",
        "\n",
        "<a id=\"q223\"></a>\n",
        "\n",
        "Save your model weights to the path `model/your_model_name` where `your_model_name` is whatever filename prefix you want. Then reload your weights from that same path.\n",
        "  - **NOTE:** If you don't intend to use it, you may leave your line of code commented out. Nothing should change if you run it after saving it though, since it will load the same weights and everything else about the model will still be in memory. If you close your notebook or restart your kernel in the future, run all the cells required to compile the model, but skip the cells that performs the fit and the save. After running the load weights cell, your previously trained model will be restored.\n",
        "  - **Answer this question in a few sentences:** Suppose you save just the weights after training for a while. If you were to load the weights again and continue training, would it work? How will it be different than continuing from a full-model save?\n",
        "    \n",
        "**Note:** Please do *not* upload your model weights with your notebook submission.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFVM_NMbx0Mv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tSH_9wox0Mw",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Bk7_apRVO5iX"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Q2.2.4 Evaluating your model</b>\n",
        "\n",
        "<a id=\"q224\"></a>\n",
        "\n",
        "Create a subplots grid with 4 rows and 3 columns. Each row will be a separate image from the test set (of your choice) and each column will consist of: Original Image, Predicted Straightened Image, Target Straightened Image. The title of the latter two should be the predicted rotation and the actual rotation. For example, a row should look something like the image shown below. This can be achieved using the provided function `rot_resize` to correct for the rotation predicted by your network.\n",
        "\n",
        "![straightened face](data/straightened.png)\n",
        "    \n",
        "**Hint:** Your network expects its input to have an explicit 'batch' dimension (the 1st dimension). If you use `take()` to get one image from your dataset, you will need to add this extra batch dimension to it in order to pass it to `predict()`. There are several ways to do this. You can check out [this SO post](https://stackoverflow.com/questions/43017017/keras-model-predict-for-a-single-image) for some ideas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWsHliY1x0My",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBcY7SbNx0Mz",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JBALLSo7O5iY"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'>\n",
        "<a id=\"q23\"></a>\n",
        "<b>2.3 Further Analysis</b>\n",
        "<hr>\n",
        "<b>Q2.3.1 Correct an image of your choosing</b>\n",
        "\n",
        "<a id=\"q231\"></a>\n",
        "\n",
        "Find an image or image(s) (not from the provided test/training sets), or make your own; it does not necessarily have to be a human face. You may rotate it yourself up to $\\pm60^\\circ$, or the face can already be naturally rotated. Resize and crop the image to 140px by 120px, load it here, and normalize it to [0.,1.] (you may use the provided `normalize_image` function) and use your network to correct it.\n",
        "\n",
        "![Confused Chris](data/chrisprattcorrection.png)\n",
        "    \n",
        "**Note:** Please do *not* upload your custom image as a separate file with your notebook submission. It is sufficient to display your results in the cell output.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptw3bIJcx0M0",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# your code here\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "wtsK4iCCO5iY"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "<div class='exercise'><b>Wrap-up</b>\n",
        "\n",
        "* In a few sentences, please describe the aspect(s) of the assignment you found most challenging. This could be conceptual and/or related to coding and implementation.\n",
        "\n",
        "* How many hours did you spend working on this assignment? Store this as an int or float in `hours_spent_on_hw`. If you worked on the project in a group, report the *average* time spent per person.\n",
        "    </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "7GLPU-7FO5iY"
      },
      "outputs": [],
      "source": [
        "hours_spent_on_hw = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Jaf0kOWiO5iY"
      },
      "outputs": [],
      "source": [
        "grader.check(\"wrapup\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBDh_FiwO5iY"
      },
      "outputs": [],
      "source": [
        "time_end = time.time()\n",
        "print(f\"It took {(time_end - time_start)/60:.2f} minutes for this notebook to run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwztL6GcO5iY"
      },
      "source": [
        "**This concludes HW4. Thank you!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln75ejPkx0M1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "otter": {
      "OK_FORMAT": true,
      "tests": {
        "wrapup": {
          "name": "wrapup",
          "points": null,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> assert float(hours_spent_on_hw), 'Please select a time in hours (int or float) to specify how long you spent on this assignment.'\n",
                  "hidden": false,
                  "locked": false
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}